# ğŸ§ ğŸ“± **AXIOM-Mobile â€” Minimal Data for Onâ€‘Device Domain Reasoning**  
### *Selection Strategies â€¢ Learning Curves â€¢ Core ML Deployment â€¢ Real Device Profiling*

<p align="center">
  <img alt="AXIOM-Mobile Banner" src="https://dummyimage.com/1200x260/0b1020/ffffff&text=AXIOM-Mobile:+Minimal+Data+for+Mobile+Reasoning" />
</p>

<p align="center">
  <b>AXIOM-Mobile</b> is a research-driven iOS/macOS system + experiment that measures the <b>minimal dataset</b> required for effective domain reasoning <b>under real mobile constraints</b>.
</p>

<p align="center">
  <i>SwiftUI + Core ML on device â€¢ Python (PyTorch/Transformers/LoRA) for training â€¢ Reproducible learning curves</i>
</p>

---

## ğŸ¯ **Research Question**
> **What is the minimal training set size (k\*) that achieves effective domain reasoning on mobile devices under strict quality + latency + energy constraints?**îˆ

### âœ… Operational Definition of â€œEffectiveâ€
A model is **effective** only if it satisfies **both** categories simultaneously:

**Quality Threshold**
- **â‰¥ 70% Exact Match (EM)** on a held-out test set  
- Alternative metrics (task-dependent): **BLEU-4 â‰¥ 0.65**, **hit@5 â‰¥ 0.80**  
- **Hallucination rate < 10%** (answers must be grounded in visible content or KG)

**Device Constraints** *(measured on iPhone 14+ and M1/M2 Mac)*
- Latency: **p50 â‰¤ 400ms**, **p95 â‰¤ 600ms** per query  
- Energy: **< 5% battery drain per hour** during continuous use  
- Model size: **< 100MB** total app footprint (models + supporting data)  
- Memory: **Peak < 500MB RAM**

---

## ğŸ§© **What AXIOM-Mobile Builds**

### ğŸ–¥ï¸ The System (On-Device App)
- **iOS/macOS app** using **SwiftUI + Core ML**  
- Captures/loads **screen content (screenshots)** and answers **natural language questions**  
- Uses **vision + text + compact knowledge graph grounding**  
- Runs **entirely on-device** with **zero network transmission**

### ğŸ§ª The Experiment (Data-Efficiency Study)
We run a controlled scaling study:
1. Curate **500** high-quality screenshot-question-answer triples  
2. Train models on progressively larger subsets:  
   **k = {10, 25, 50, 100, 250, 500}**
3. Compare four selection strategies:  
   **RAND**, **UNC**, **DIV**, **KG-guided**  
4. Deploy each model to iPhone/Mac and measure:  
   **quality + latency + energy + memory** 
5. Identify **k\*** per strategy and determine which reaches **k\*** fastest 

---

## ğŸš€ **Key Features**

### ğŸ§  Data-Efficient Training
- **LoRA (PEFT)** fine-tuning for efficient adaptation
- **24 model variants** (4 strategies Ã— 6 budgets), plus **multiple seeds** for rigor

### ğŸ§­ Selection Strategies
| Strategy | Idea | Implementation Sketch |
|---|---|---|
| **RAND** | Uniform random sampling | Sample k from pool |
| **UNC** | Pick examples with highest uncertainty | Entropy / margin |
| **DIV** | Maximize coverage of feature space | k-center / clustering |
| **KG-guided** | Cover underrepresented KG regions | Entity/relation coverage |

(Defined explicitly in project methodology.)

### ğŸ“¦ Core ML Deployment + Compression
- **PyTorch â†’ Core ML (.mlpackage)** conversion
- Accept conversion only if **accuracy drop â‰¤ 3%** 
- **Post-training quantization** targeting ~4Ã— size reduction 

### ğŸ“ Real Device Profiling (Not Just Offline Metrics)
- Evaluate on **iPhone/Mac hardware** with airplane mode enabled 
- Log to **CSV** automatically for analysis
- Analyze learning curves with power-law fits and compare strategies statistically

---

## ğŸ§° **Installation**

### âœ… Python (Training + Evaluation)
```bash
python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -e "ml[dev]"
```

> The repository does **not** store raw screenshots. See `data/README.md` for dataset handling and split manifests.

### âœ… iOS/macOS (App)
1. Open `app/AXIOMMobile.xcodeproj` (or `.xcodeworkspace`) in Xcode  
2. Select an iPhone simulator or your device  
3. Build + Run

---

## ğŸ” **1) Prepare Dataset Manifests**
Dataset format follows a simple triple:
- screenshot â†’ question â†’ answer  
with optional metadata (bbox, difficulty, KG entities).

Expected (recommended) split proportions:  
- **Test 20%**, **Validation 10%**, **Pool 70%**  

**Example layout**
```
data/
  manifests/
    pool.jsonl
    val.jsonl
    test.jsonl
  schema/
    example.schema.json
```

---

## ğŸ¤– **2) Train Models at Label Budget k**
Train one strategy at one budget (example):
```bash
python -m axiom.train \
  --strategy RAND \
  --k 50 \
  --seed 0 \
  --outdir results/runs/RAND_k50_seed0
```

Train the full sweep (24 configs):
```bash
python -m axiom.sweep \
  --strategies RAND,UNC,DIV,KG \
  --budgets 10,25,50,100,250,500 \
  --seeds 0,1,2 \
  --outdir results/runs
```

Outputs (per run):
- `metrics_offline.json`  
- `model_checkpoint/`  
- `export_coreml/AXIOMMobile.mlpackage`

---

## ğŸ“¦ **3) Export to Core ML**
```bash
python -m axiom.export_coreml \
  --run results/runs/RAND_k50_seed0 \
  --outdir artifacts/coreml/RAND_k50_seed0
```

Acceptance gate:
- Export is accepted only if **accuracy drop â‰¤ 3%** after conversion

---

## ğŸ“± **4) On-Device Evaluation (iPhone/Mac)**
AXIOM-Mobile measures on-device performance across **all models**: 
- **Exact Match, BLEU-4, hallucination rate, grounding accuracy**  
- **Latency p50/p95, peak memory, battery drain via Instruments**

The app writes a CSV like:
```
timestamp,device,model_id,strategy,k,seed,em,bleu,lat_p50_ms,lat_p95_ms,mem_peak_mb,battery_pct_per_hr
```

---

## ğŸ§ª **5) Run Tests**
```bash
pytest -q
```

---

## ğŸ—‚ï¸ **Project Structure**
```
axiom-mobile/
â”œâ”€â”€ app/                      # SwiftUI + Core ML app (iOS/macOS)
â”‚   â””â”€â”€ AXIOMMobile/
â”œâ”€â”€ ml/                       # Python package (training, selection, eval, export)
â”‚   â”œâ”€â”€ src/axiom/
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ selection/
â”‚   â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ eval/
â”‚   â”‚   â””â”€â”€ export/
â”‚   â””â”€â”€ scripts/
â”œâ”€â”€ kg/                       # Compact KG (JSON/SQLite) embedded in app
â”œâ”€â”€ data/                     # Manifests + schema (no raw screenshots)
â”œâ”€â”€ results/                  # Run outputs (gitignored except README)
â”œâ”€â”€ docs/                     # Spec, milestones, paper outline
â””â”€â”€ README.md
```

---

## ğŸ§­ **Roadmap (Semester Phases)**
- **Phase 1 (Weeks 1â€“4):** Dataset curation + QC (2 annotators, Îº â‰¥ 0.75), KG assembly  
- **Phase 2 (Weeks 5â€“6):** Baseline model selection; verify constraints on device
- **Phase 3 (Weeks 7â€“10):** Implement selection strategies and train 24 models (+ seeds)  
- **Phase 4 (Weeks 11â€“12):** Compression + Core ML conversion + accuracy gate 
- **Phase 5 (Weeks 13â€“14):** On-device evaluation with Instruments logging
- **Phase 6 (Weeks 15â€“16):** Analysis + publication-quality write-up

---

## ğŸ” Data Policy
- **No raw screenshots** are stored in Git.  
- Use `data/manifests/` + secure storage (private drive / DVC / S3) for images.  
- All on-device evaluation runs in **airplane mode** to preserve privacy and measurement validity.

---

## âœï¸ Citation (Draft)
If you build on this work, please cite (placeholder):
```bibtex
@misc{axiom_mobile_2026,
  title        = {AXIOM-Mobile: Minimal Data for On-Device Domain Reasoning},
  author       = {Last Name, First Name},
  year         = {2026},
  howpublished = {GitHub repository}
}
```

---

## ğŸ“„ License
MIT License Â© 2026  
Annie Boltwood, Mahim Chaudhary & Ariel Tyson
